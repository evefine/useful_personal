{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evefine/useful_personal/blob/main/analysis_funcs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c46dc0b-8ac7-46dd-a981-f148cdb31a7f",
      "metadata": {
        "tags": [],
        "id": "1c46dc0b-8ac7-46dd-a981-f148cdb31a7f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import importlib\n",
        "import gpcrmining.gpcrdb as db\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from bokeh.plotting import figure, show, output_notebook\n",
        "from bokeh.models import ColumnDataSource, LabelSet, HoverTool\n",
        "from bokeh.layouts import layout\n",
        "from bokeh.io import curdoc\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import functools\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import functools\n",
        "\n",
        "# Assuming db and other required libraries are imported correctly\n",
        "\n",
        "\n",
        "def collect_pdb_filenames(path):\n",
        "    \"\"\"\n",
        "    This function remains largely unchanged as its operation is already efficient for its purpose.\n",
        "    \"\"\"\n",
        "    pdb_filenames = []\n",
        "    gpcr_names = []\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith('.pdb'):\n",
        "            pdb_filenames.append(filename)\n",
        "            gpcr_names.append(filename[:-9])\n",
        "    return pdb_filenames, gpcr_names\n",
        "\n",
        "\n",
        "def get_res_info(gpcr):\n",
        "    \"\"\"\n",
        "    Modified to process a single GPCR at a time to facilitate parallel processing.\n",
        "    A caching mechanism can be added here if db.get_residue_info(gpcr) is costly.\n",
        "    \"\"\"\n",
        "    return gpcr, db.get_residue_info(gpcr)\n",
        "\n",
        "\n",
        "def parallel_get_res_info(list_gpcrs):\n",
        "    \"\"\"\n",
        "    Uses multiprocessing to fetch residue info in parallel.\n",
        "    \"\"\"\n",
        "    with Pool(processes=cpu_count()) as pool:\n",
        "        res_info = pool.map(get_res_info, list_gpcrs)\n",
        "    return dict(res_info)\n",
        "\n",
        "\n",
        "def find_matching_values(nested_list, generic_res):\n",
        "    \"\"\"\n",
        "    Searches through a nested list for sublists whose last entry starts with\n",
        "    the specified generic numbers and optionally matches an atom type if specified.\n",
        "    Returns a list of tuples with the integer values from the second position of those sublists\n",
        "    and the atom type ('CA' if not specified), ordered according to generic_res.\n",
        "\n",
        "    Args:\n",
        "    - nested_list: A nested list expected to have the format ['string', integer, 'string', 'string'].\n",
        "    - generic_res: A list of strings with generic numbers such as '2.46' or '2.46CZ' for specific atom types.\n",
        "\n",
        "    Returns:\n",
        "    - A list of tuples (integer, atom type) from matching sublists, ordered as per generic_res.\n",
        "    \"\"\"\n",
        "    matching_values = []\n",
        "    for res in generic_res:\n",
        "        generic_number = res[:4]  # Always capture the first 4 characters as the generic identifier\n",
        "        atom_type = 'CA'  # Default atom type\n",
        "        if len(res) > 4:  # If there are more characters, assume they specify an atom type\n",
        "            atom_type = res[4:]\n",
        "\n",
        "        found = False\n",
        "        for sublist in nested_list:\n",
        "            if sublist[-1].startswith(generic_number):\n",
        "                matching_values.append((sublist[1], atom_type))\n",
        "                found = True\n",
        "                break  # Found the matching generic number (and atom type)\n",
        "        if not found:\n",
        "            matching_values.append(None)  # Append None if no match was found\n",
        "\n",
        "    return matching_values\n",
        "\n",
        "\n",
        "def find_coordinates_once(path, res_nums_and_atom_types):\n",
        "    found_coords_map = {}\n",
        "    with open(path, 'r') as pdb_file:\n",
        "        for line in pdb_file:\n",
        "            if line.startswith(\"ATOM\"):\n",
        "                resnum = int(line[22:26].strip())\n",
        "                atom = line[13:15].strip()\n",
        "                if (resnum, atom) in res_nums_and_atom_types:\n",
        "                    x = float(line[30:38].strip())\n",
        "                    y = float(line[38:46].strip())\n",
        "                    z = float(line[46:54].strip())\n",
        "                    found_coords_map[(resnum, atom)] = (x, y, z)\n",
        "\n",
        "    # Order the results based on the input list\n",
        "    ordered_coords = [found_coords_map.get(pair) for pair in res_nums_and_atom_types]\n",
        "\n",
        "    return ordered_coords\n",
        "\n",
        "\n",
        "def compute_distance(coord1, coord2):\n",
        "    \"\"\"\n",
        "    Compute the Euclidean distance between two points in 3D space.\n",
        "\n",
        "    Parameters:\n",
        "    - coord1: Coordinates (x, y, z) of the first point.\n",
        "    - coord2: Coordinates (x, y, z) of the second point.\n",
        "\n",
        "    Returns:\n",
        "    The Euclidean distance between the two points.\n",
        "    \"\"\"\n",
        "    if coord1 == None or coord2 == None:\n",
        "        return None\n",
        "    return round(math.sqrt(sum((c1 - c2) ** 2 for c1, c2 in zip(coord1, coord2))),3)\n",
        "\n",
        "\n",
        "def write_csv(lines, csv_path):\n",
        "    \"\"\"\n",
        "    Writes a nested list to a CSV file.\n",
        "\n",
        "    Args:\n",
        "    - lines: Nested list where each sublist represents a row in the CSV.\n",
        "    - csv_path: Path to the CSV file to be written.\n",
        "    \"\"\"\n",
        "    # Open the file at csv_path in write mode ('w')\n",
        "    with open(csv_path, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        # Write each sublist in lines as a row in the CSV file\n",
        "        writer.writerows(lines)\n",
        "\n",
        "\n",
        "def process_file(pdb_file, path, res_info, pairs_flat):\n",
        "    \"\"\"\n",
        "    Process a single file. This function is designed to run in parallel.\n",
        "    \"\"\"\n",
        "    dists = [pdb_file[:-9], pdb_file[:-4][-4:]]\n",
        "    coords = find_coordinates_once(path + pdb_file, find_matching_values(res_info[pdb_file[:-9]], pairs_flat))\n",
        "\n",
        "    for i in range(int(len(coords) / 2)):\n",
        "        dists.append(compute_distance(coords[i * 2], coords[i * 2 + 1]))\n",
        "\n",
        "    return dists\n",
        "\n",
        "\n",
        "def get_distances(path, pairs, csv_name):\n",
        "    filenames, gpcr_names = collect_pdb_filenames(path)\n",
        "    res_info = parallel_get_res_info(set(gpcr_names))  # Use set to ensure uniqueness\n",
        "\n",
        "    pairs_flat = [item for sublist in pairs for item in sublist]\n",
        "    lines = [['GPCR name', 'PDB code'] + [f'{pair[0]}-{pair[1]}' for pair in pairs]]\n",
        "\n",
        "    # Use functools.partial to create a partial function with some arguments pre-filled\n",
        "    process_file_partial = functools.partial(process_file, path=path, res_info=res_info, pairs_flat=pairs_flat)\n",
        "\n",
        "    # Processing files in parallel\n",
        "    with Pool(processes=cpu_count()) as pool:\n",
        "        results = pool.map(process_file_partial, filenames)\n",
        "\n",
        "    lines.extend(results)\n",
        "    write_csv(lines, paht + csv_name + '.csv')\n",
        "    print('done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27237995-d598-4a06-925a-33bde97455e0",
      "metadata": {
        "id": "27237995-d598-4a06-925a-33bde97455e0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}